---
layout: post
title: YOLOV5 notes
date: 2021-06-25
Author: Lu
categories: 
tags: [notes]
toc: true
pinned: true
--- 

## Mosaic数据增强
Mosaic是参考2019年底提出的CutMix数据增强的方式，但CutMix只使用了两张图片进行拼接，而Mosaic数据增强则采用了4张图片，随机缩放、随机裁剪、随机排布的方式进行拼接。

![alt pic](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%871.png)

这里首先要了解为什么要进行Mosaic数据增强呢？

在平时项目训练时，小目标的AP一般比中目标和大目标低很多。而Coco数据集中也包含大量的小目标，但比较麻烦的是小目标的分布并不均匀。针对这种状况，Yolov5的作者采用了Mosaic数据增强的方式。主要有几个优点：
1. 丰富数据集：随机使用4张图片，随机缩放，再随机分布进行拼接，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。
2. 减少GPU：可能会有人说，随机缩放，普通的数据增强也可以做，但作者考虑到很多人可能只有一个GPU，因此Mosaic增强训练时，可以直接计算4张图片的数据，使得Mini-batch大小并不需要很大，一个GPU就可以达到比较好的效果。

## 自适应锚框计算
在Yolo算法中，针对不同的数据集，都会有初始设定长宽的锚框。

在网络训练中，网络在初始锚框的基础上输出预测框，进而和真实框groundtruth进行比对，计算两者差距，再反向更新，迭代网络参数。

在Yolov3、Yolov4中，训练不同的数据集时，计算初始锚框的值是通过单独的程序运行的。

但Yolov5中将此功能嵌入到代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。当然，如果觉得计算的锚框效果不是很好，也可以在代码中将自动计算锚框功能关闭。
## 自适应图片缩放
在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。

比如Yolo算法中常用416\*416，608\*608等尺寸，比如对下面**800*600**的图像进行缩放。

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%872.jpg)

但Yolov5代码中对此进行了改进，也是Yolov5推理速度能够很快的一个不错的trick。作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多,则存在信息冗余,影响推理速度。因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像自适应的添加最少的黑边。
 
 ![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%873.jpg)
 
第一步：计算缩放比例。原始缩放尺寸是**416*416**，都除以原始图像的尺寸后，可以得到**0.52**和**0.69**两个缩放系数，选择小的缩放系数。
 
第二步：计算缩放后的尺寸。原始图片的长宽都乘以**最小**的缩放系数0.52，宽变成了**416**，而高变成了**312**。
 
第三步：计算黑边填充数值。将**416-312=104**，得到原本需要填充的高度。再采用numpy中np.mod取余数的方式，得到**8**个像素，再除以**2**，即得到图片高度两端需要填充的数值。

此外，需要注意的是：
1. Yolov5中填充的是灰色，即（114,114,114），都是一样的效果。
2. **训练**时没有采用缩减黑边的方式，还是采用传统填充的方式，即缩放到416*416大小。只是在**测试**，使用模型推理时，才采用缩减黑边的方式，提高目标检测，推理的速度。
3. 为什么np.mod函数的后面用32？因为Yolov5的网络经过5次下采样，而2的5次方，等于32。所以至少要去掉32的倍数，再进行取余。
## Backbone
![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%874.png)
* Focus结构。将图像相邻的四个位置进行堆叠，聚焦**wh**维度信息到**c**通道空间，每个点的提高感受野，并减少原始信息的丢失，有点类似于进行了*kernal_size=2，stride=2*的卷积操作。

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%875.jpg)
* CSP结构。通过**残差连接**提升模型的特征抽取能力和网络层数。

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%876.jpg)
* 空间金字塔池化。**集成不同感受野的池化特征。
## Neck
![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%878.jpg)
![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%879.jpg)
## 输出端
* Bounding box损失函数。
Yolov5中采用其中的GIOU_Loss做Bounding box的损失函数。

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%8710.jpg)

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%8711.jpg)

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%8712.jpg)

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%8713.jpg)
* 使用FocalLoss：参数alpha平衡正负样本和参数gamma控制难易样本的权重。计算损失：通过计算**GIOU、回归损失、分类损失加权求和**得到最终损失。

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%8714.jpg)

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%8715.jpg)
* NMS非极大值抑制。在目标检测的后处理过程中，针对很多目标框的筛选，通常需要nms操作。因为CIOU_Loss中包含影响因子v，涉及groudtruth的信息，而测试推理时，是没有groundtruth的。所以Yolov4在DIOU_Loss的基础上采用**DIOU_nms**的方式，而Yolov5中采用**加权nms**的方式。
## Yolov5四种网络的深度
* 深度。

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%8717.png)
* 宽度。

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%8718.png)

![](https://github.com/lulugai/Blog/blob/master/images/%E5%9B%BE%E7%89%8719.jpg)
## Metric [(参考这里)](https://zhuanlan.zhihu.com/p/70306015)
* mAP（平均准确度均值）

mAP: mean Average Precision, 即各类别AP的平均值

AP: PR曲线下面积 

PR曲线: Precision-Recall曲线

Precision: TP / (TP + FP)

Recall: TP / (TP + FN)

TP: IoU>0.5的检测框数量（同一Ground Truth只计算一次）

FP: IoU<=0.5的检测框，或者是检测到同一个GT的多余检测框的数量

FN: 没有检测到的GT的数量

**注意：一般来说mAP针对整个数据集而言的；AP针对数据集中某一个类别而言的；而percision和recall针对单张图片某一类别的。**
* mAP的具体计算

（1）在VOC2010以前，只需要选取当Recall >= 0, 0.1, 0.2, ..., 1共11个点时的Precision最大值，然后AP就是这11个Precision的平均值，map就是所有类别AP值的平均。

（2）在VOC2010及以后，需要针对每一个不同的Recall值（包括0和1），选取其大于等于这些Recall值时的Precision最大值，然后计算PR曲线下面积作为AP值，map就是所有类别AP值的平均。

（3）COCO数据集，设定多个IOU阈值（0.5-0.95,0.05为步长），在每一个IOU阈值下都有某一类别的AP值，然后求不同IOU阈值下的AP平均，就是所求的最终的某类别的AP值。
*	如何判断TP,FP,FN

拿单张图片来说吧，首先遍历图片中ground truth对象，然后提取我们要计算的某类别的gt_objects，之后读取我们通过检测器检测出的这种类别的检测框（其他类别的先不管），接着**过滤掉置信度分数低于置信度阈值的框（也有的未设置信度阈值）**，将剩下的检测框按**置信度分数**从高到低排序，最先判断置信度分数最高的检测框与gt_bbox的iou是否大于**iou阈值**，若iou大于设定的iou阈值即判断为TP，将此gt_bbox标记为已检测(**后续的同一个GT的多余检测框都视为FP,这就是为什么先要按照置信度分数从高到低排序，置信度分数最高的检测框最先去与iou阈值比较，若大于iou阈值，视为TP，后续的同一个gt对象的检测框都视为FP**），iou小于阈值的，直接规划到FP中去。这里置信度分数不同的论文可能对其定义不一样，一般指分类置信度的居多，也就是预测框中物体属于某一个类别的概率。

关于图片中FN的统计就比较简单了，图片中某类别一共有多少个gt我们是知道的，减去TP的个数，剩下的就是FN的个数了。
* 速度指标

一般来说目标检测中的速度评价指标有：（1）**FPS**，检测器每秒能处理图片的张数（2）检测器处理每张图片所需要的时间。
*	FLOPs和FLOPS区分

先解释一下**FLOPs**：floating point operations 指的是浮点运算次数，理解为计算量，可以用来衡量算法/模型的复杂度。此处区分一下**FLOPS**（**全部大写**），FLOPS指的是每秒运算的浮点数，理解为计算速度，衡量一个硬件的标准。我们要的是衡量模型的复杂度的指标，所以选择FLOPs。
